{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOexUivXzPdxaPGWgeSqJDE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/QaziSaim/Fine-Tune-Projects/blob/main/Attention_Mechenisum.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Luckly I already had saved my tokenizer now i am going to load it"
      ],
      "metadata": {
        "id": "D9ONGvHuaSwQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow\n",
        "import re\n",
        "import string\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Layer"
      ],
      "metadata": {
        "id": "x3Ho0k3dIH-B"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/eng_to_fra.csv')"
      ],
      "metadata": {
        "id": "mryvh0jKIPsa"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eng_tokenizer = Tokenizer()\n",
        "eng_tokenizer.fit_on_texts(df.eng_texts)\n",
        "eng_sequence = eng_tokenizer.texts_to_sequences(df.eng_texts)"
      ],
      "metadata": {
        "id": "h42_iX1yIYrn"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fra_tokenizer = Tokenizer()\n",
        "fra_tokenizer.fit_on_texts(df.fra_texts)\n",
        "fra_sequence = fra_tokenizer.texts_to_sequences(df.fra_texts)"
      ],
      "metadata": {
        "id": "n_1QAzleI0JK"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eng_vocab_size = len(eng_tokenizer.word_index) +1\n",
        "fra_vocab_size = len(fra_tokenizer.word_index) + 1\n"
      ],
      "metadata": {
        "id": "xwkLA_jVJIuu"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_eng_len = max(len(x) for x in eng_sequence)\n",
        "max_fra_len = max(len(x) for x in fra_sequence)"
      ],
      "metadata": {
        "id": "NyswjXAyJgzH"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_input = pad_sequences(eng_sequence, maxlen=max_eng_len, padding='post')\n",
        "decoder_input = pad_sequences([s[:-1] for s in fra_sequence],maxlen=max_fra_len-1,padding='post')\n",
        "decoder_target = pad_sequences([s[1:] for s in fra_sequence],maxlen=max_fra_len-1,padding='post')"
      ],
      "metadata": {
        "id": "N1VFzt2QJu_0"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_input_size = len(eng_tokenizer.word_index) + 1\n",
        "vocab_target_size = len(fra_tokenizer.word_index) + 1"
      ],
      "metadata": {
        "id": "Yhco1oE1Jw5U"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 128\n",
        "units = 256\n",
        "batch = 16\n",
        "EPOCHES = 10"
      ],
      "metadata": {
        "id": "327YTc2OjXjv"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KnZabuRvj6Ng"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BahdanauAttetion(Layer):\n",
        "  def __init__(self,units):\n",
        "    super().__init__()\n",
        "    self.w1 =  Dense(units)\n",
        "    self.w2 = Dense(units)\n",
        "    self.V = Dense(1)\n",
        "\n",
        "  def call(self,query,value):\n",
        "    query_with_time_axis = tensorflow.expand_dims(query, 1)\n",
        "    score = self.V(tensorflow.nn.tanh(self.w1(value) + self.w2(query_with_time_axis)))\n",
        "    attention_weights = tensorflow.nn.softmax(score, axis=1)\n",
        "    context_vector = tensorflow.reduce_sum(attention_weights * value, axis=1)\n",
        "    return context_vector, tensorflow.squeeze(attention_weights, -1)\n"
      ],
      "metadata": {
        "id": "h56psyRojleO"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units):\n",
        "    super(Encoder,self).__init__()\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = Embedding(vocab_size,embedding_dim)\n",
        "    self.lstm = LSTM(enc_units, return_sequences=True, return_state=True)\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.embedding(x)\n",
        "    output, state_h, state_c = self.lstm(x)\n",
        "    return output, state_h, state_c\n",
        ""
      ],
      "metadata": {
        "id": "82NZU1kXkcH5"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units):\n",
        "    super(Decoder,self).__init__()\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = Embedding(vocab_size,embedding_dim)\n",
        "    self.attention = BahdanauAttetion(dec_units)\n",
        "\n",
        "    self.cell = tensorflow.keras.layers.LSTMCell(dec_units)\n",
        "    self.dence  = Dense(vocab_size)\n",
        "\n",
        "  def call(self, dec_input, enc_output, initial_state):\n",
        "    batch_size = tensorflow.shape(dec_input)[0]\n",
        "    dec_seq_len = tensorflow.shape(dec_input)[1]\n",
        "\n",
        "    embedding = self.embedding(dec_input)\n",
        "    outputs = []\n",
        "    attention_weights_history = []\n",
        "\n",
        "    states = initial_state\n",
        "\n",
        "    for t in range(dec_seq_len):\n",
        "      x_t = embedding[:, t, :]\n",
        "\n",
        "      context_vector, attn_weight = self.attention(states[0], enc_output)\n",
        "\n",
        "      x_and_context = tensorflow.concat([x_t, context_vector], axis=-1)\n",
        "      output, states = self.cell(x_and_context, states)\n",
        "      logits = self.dense(output)\n",
        "      outputs.append(tensorflow.expand_dims(logits,1))\n",
        "      attention_weights_history.append(tensorflow.expand_dims(attn_weight, 1))\n",
        "\n",
        "    logits_seq = tensorflow.concat(outputs, axis=1)\n",
        "    attention_history = tensorflow.concat(attention_weights_history, axis = 1)\n",
        "    return logits_seq, states, attention_history\n",
        "\n"
      ],
      "metadata": {
        "id": "yg97kl0Bqsp1"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Seq2Seq(Model):\n",
        "  def __init__(self, encoder,decoder):\n",
        "    super(Seq2Seq,self).__init__()\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "\n",
        "  def call(self, inputs):\n",
        "    enc_in, dec_in = inputs\n",
        "\n",
        "    enc_output,enc_h,enc_c = self.encoder(enc_in)\n",
        "    logits, _, attn = self.decoder(dec_in, enc_output, (enc_h, enc_c))\n",
        "    return logits\n"
      ],
      "metadata": {
        "id": "QhyWu8R9tlFc"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = Encoder(vocab_input_size, embedding_dim, units)\n",
        "decoder = Decoder(vocab_target_size, embedding_dim, units)\n",
        "model = Seq2Seq(encoder,decoder)"
      ],
      "metadata": {
        "id": "vy6RhqwzulMc"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_object = tensorflow.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')"
      ],
      "metadata": {
        "id": "0jpOuKZAu720"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_function(real, pred):\n",
        "  mask = tensorflow.cast(tensorflow.not_equal(real, 0), dtype=tensorflow.float32)\n",
        "  loss_ = loss_object(real, pred)\n",
        "  loss_ *= mask\n",
        "  return tensorflow.reduce_sum(loss_) / (tensorflow.reduce_sum(mask) + le-9)"
      ],
      "metadata": {
        "id": "XRfTLWugvIKH"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',loss=loss_function, metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "k3g6ppe3v5BD"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "1_pDdW85wCOC",
        "outputId": "eecaafd8-d857-40c7-f274-45d578b64c38"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"seq2_seq\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"seq2_seq\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ encoder (\u001b[38;5;33mEncoder\u001b[0m)               │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ decoder (\u001b[38;5;33mDecoder\u001b[0m)               │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ encoder (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Encoder</span>)               │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ decoder (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Decoder</span>)               │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HPlsY6NzwDH5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}