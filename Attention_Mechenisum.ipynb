{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOI6AdwQnVPhhmXL2ImehOl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/QaziSaim/Fine-Tune-Projects/blob/main/Attention_Mechenisum.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Luckly I already had saved my tokenizer now i am going to load it"
      ],
      "metadata": {
        "id": "D9ONGvHuaSwQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow\n",
        "import re\n",
        "import string\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense"
      ],
      "metadata": {
        "id": "x3Ho0k3dIH-B"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/eng_to_fra.csv')"
      ],
      "metadata": {
        "id": "mryvh0jKIPsa"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eng_tokenizer = Tokenizer()\n",
        "eng_tokenizer.fit_on_texts(df.eng_texts)\n",
        "eng_sequence = eng_tokenizer.texts_to_sequences(df.eng_texts)"
      ],
      "metadata": {
        "id": "h42_iX1yIYrn"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fra_tokenizer = Tokenizer()\n",
        "fra_tokenizer.fit_on_texts(df.fra_texts)\n",
        "fra_sequence = fra_tokenizer.texts_to_sequences(df.fra_texts)"
      ],
      "metadata": {
        "id": "n_1QAzleI0JK"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eng_vocab_size = len(eng_tokenizer.word_index) +1\n",
        "fra_vocab_size = len(fra_tokenizer.word_index) + 1\n"
      ],
      "metadata": {
        "id": "xwkLA_jVJIuu"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_eng_len = max(len(x) for x in eng_sequence)\n",
        "max_fra_len = max(len(x) for x in fra_sequence)"
      ],
      "metadata": {
        "id": "NyswjXAyJgzH"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_input = pad_sequences(eng_sequence, maxlen=max_eng_len, padding='post')\n",
        "decoder_input = pad_sequences([s[:-1] for s in fra_sequence],maxlen=max_fra_len-1,padding='post')\n",
        "decoder_target = pad_sequences([s[1:] for s in fra_sequence],maxlen=max_fra_len-1,padding='post')"
      ],
      "metadata": {
        "id": "N1VFzt2QJu_0"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_input_size = len(eng_tokenizer.word_index) + 1\n",
        "vocab_target_size = len(fra_tokenizer.word_index) + 1"
      ],
      "metadata": {
        "id": "Yhco1oE1Jw5U"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding = 128\n",
        "units = 256\n",
        "batch = 16\n",
        "EPOCHES = 10"
      ],
      "metadata": {
        "id": "327YTc2OjXjv"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KnZabuRvj6Ng"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Layer\n",
        "class BahdanauAttetion(Layer):\n",
        "  def __init__(self,units):\n",
        "    super().__init__()\n",
        "    self.w1 =  Dense(units)\n",
        "    self.w2 = Dense(units)\n",
        "    self.V = Dense(1)\n",
        "\n",
        "  def call(self,query,value):\n",
        "    query_with_time_axis = tensorflow.expand_dims(query, 1)\n",
        "    score = self.V(tensorflow.nn.tanh(self.w1(value) + self.w2(query_with_time_axis)))\n",
        "    attention_weights = tensorflow.nn.softmax(score, axis=1)\n",
        "    context_vector = tensorflow.reduce_sum(attention_weights * value, axis=1)\n",
        "    return context_vector, tensorflow.squeeze(attention_weights, -1)\n"
      ],
      "metadata": {
        "id": "h56psyRojleO"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "82NZU1kXkcH5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}